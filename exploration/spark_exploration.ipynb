{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('data_quality')\\\n",
    "    .config(\"spark.master\", 'local[4]')\\\n",
    "    .config(\"spark.shuffle.sql.partitions\", 1)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.10.106:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_quality</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c9f7990640>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'name', 'x']\n",
    "l = [\n",
    "    (1, 'A', 'a'),\n",
    "    (2, 'B', 'b'),\n",
    "    (3, 'C', 'c'),\n",
    "    (4, 'D', 'd'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, schema=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|  x|\n",
      "+---+----+---+\n",
      "|  1|   A|  a|\n",
      "|  2|   B|  b|\n",
      "|  3|   C|  c|\n",
      "|  4|   D|  d|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../landing_zone/persistent/acled_20211224.csv', sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232494"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data_id: string (nullable = true)\n",
      " |-- iso: string (nullable = true)\n",
      " |-- event_id_cnty: string (nullable = true)\n",
      " |-- event_id_no_cnty: string (nullable = true)\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- time_precision: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- sub_event_type: string (nullable = true)\n",
      " |-- actor1: string (nullable = true)\n",
      " |-- assoc_actor_1: string (nullable = true)\n",
      " |-- inter1: string (nullable = true)\n",
      " |-- actor2: string (nullable = true)\n",
      " |-- assoc_actor_2: string (nullable = true)\n",
      " |-- inter2: string (nullable = true)\n",
      " |-- interaction: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- admin1: string (nullable = true)\n",
      " |-- admin2: string (nullable = true)\n",
      " |-- admin3: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- geo_precision: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- source_scale: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- fatalities: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- iso3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|iso3|\n",
      "+----+\n",
      "| PSE|\n",
      "| HTI|\n",
      "| POL|\n",
      "| LVA|\n",
      "| BRB|\n",
      "| JAM|\n",
      "| ZMB|\n",
      "| BRA|\n",
      "| ARM|\n",
      "| MOZ|\n",
      "| CUB|\n",
      "| JOR|\n",
      "| SOM|\n",
      "| FRA|\n",
      "| ABW|\n",
      "| TCA|\n",
      "| COD|\n",
      "| BRN|\n",
      "| BOL|\n",
      "| URY|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('iso3').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+----------------+----------+----+--------------+----------+--------------+------+-------------+------+------+-------------+------+-----------+------+-------+------+------+------+--------+--------+---------+-------------+------+------------+-----+----------+---------+----+\n",
      "|data_id|iso|event_id_cnty|event_id_no_cnty|event_date|year|time_precision|event_type|sub_event_type|actor1|assoc_actor_1|inter1|actor2|assoc_actor_2|inter2|interaction|region|country|admin1|admin2|admin3|location|latitude|longitude|geo_precision|source|source_scale|notes|fatalities|timestamp|iso3|\n",
      "+-------+---+-------------+----------------+----------+----+--------------+----------+--------------+------+-------------+------+------+-------------+------+-----------+------+-------+------+------+------+--------+--------+---------+-------------+------+------------+-----+----------+---------+----+\n",
      "|      0|  0|            0|               0|         0|   0|             0|         0|             0|     0|            0|     0|     0|            0|     0|          0|     0|      0|     6|     0|     1|       3|       0|        0|            0|     0|           0|    0|         0|        0|   0|\n",
      "+-------+---+-------------+----------------+----------+----+--------------+----------+--------------+------+-------------+------+------+-------------+------+-----------+------+-------+------+------+------+--------+--------+---------+-------------+------+------------+-----+----------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dg = pd.read_csv('../landing_zone/persistent/acled_20211224.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_id                   0\n",
       "iso                       0\n",
       "event_id_cnty             0\n",
       "event_id_no_cnty          0\n",
       "event_date                0\n",
       "year                      0\n",
       "time_precision            0\n",
       "event_type                0\n",
       "sub_event_type            0\n",
       "actor1                    0\n",
       "assoc_actor_1        765942\n",
       "inter1                    0\n",
       "actor2               602015\n",
       "assoc_actor_2       1073485\n",
       "inter2                    0\n",
       "interaction               0\n",
       "region                    0\n",
       "country                   0\n",
       "admin1                   84\n",
       "admin2                48240\n",
       "admin3               643975\n",
       "location                  0\n",
       "latitude                  0\n",
       "longitude                 0\n",
       "geo_precision             0\n",
       "source                    0\n",
       "source_scale              0\n",
       "notes                    19\n",
       "fatalities                0\n",
       "timestamp                 0\n",
       "iso3                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|      null|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as funcs\n",
    "\n",
    "df.groupBy(df.columns)\\\n",
    "    .count()\\\n",
    "    .where(funcs.col('count') > 1)\\\n",
    "    .select(funcs.sum('count'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.duplicated().sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b65f8c67370667d0adc9e610192d3d771ab29a08e3746b752a6b11121b80b184"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('adsdb': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
